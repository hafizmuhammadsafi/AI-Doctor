{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-03T16:30:01.471410Z",
     "iopub.status.busy": "2025-02-03T16:30:01.471070Z",
     "iopub.status.idle": "2025-02-03T16:31:58.434379Z",
     "shell.execute_reply": "2025-02-03T16:31:58.433267Z",
     "shell.execute_reply.started": "2025-02-03T16:30:01.471354Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/ahsannadir/llama-3-8b-instruct-aidoctor/resolve/main/unsloth.Q4_K_M.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T16:31:58.435982Z",
     "iopub.status.busy": "2025-02-03T16:31:58.435634Z",
     "iopub.status.idle": "2025-02-03T16:32:15.349534Z",
     "shell.execute_reply": "2025-02-03T16:32:15.348414Z",
     "shell.execute_reply.started": "2025-02-03T16:31:58.435943Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -U xformers --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install --no-deps packaging ninja einops flash-attn trl peft accelerate bitsandbytes\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T16:32:15.351033Z",
     "iopub.status.busy": "2025-02-03T16:32:15.350758Z",
     "iopub.status.idle": "2025-02-03T16:32:41.689260Z",
     "shell.execute_reply": "2025-02-03T16:32:41.688350Z",
     "shell.execute_reply.started": "2025-02-03T16:32:15.351006Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install llama-cpp-python\n",
    "!pip install streamlit\n",
    "!pip install chainlit\n",
    "!pip install pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T16:48:10.358230Z",
     "iopub.status.busy": "2025-02-03T16:48:10.357890Z",
     "iopub.status.idle": "2025-02-03T16:48:10.655299Z",
     "shell.execute_reply": "2025-02-03T16:48:10.654500Z",
     "shell.execute_reply.started": "2025-02-03T16:48:10.358203Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
     ]
    }
   ],
   "source": [
    "!ngrok authtoken 2sJRycBarS7IUUpygwqRIlrxIKP_2ah5aUU1hsj8oWzF83SuA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T16:48:12.093183Z",
     "iopub.status.busy": "2025-02-03T16:48:12.092873Z",
     "iopub.status.idle": "2025-02-03T16:48:12.098093Z",
     "shell.execute_reply": "2025-02-03T16:48:12.097046Z",
     "shell.execute_reply.started": "2025-02-03T16:48:12.093155Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T16:56:12.533975Z",
     "iopub.status.busy": "2025-02-03T16:56:12.533634Z",
     "iopub.status.idle": "2025-02-03T16:56:12.540455Z",
     "shell.execute_reply": "2025-02-03T16:56:12.539534Z",
     "shell.execute_reply.started": "2025-02-03T16:56:12.533950Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile ai-doctor.py\n",
    "import chainlit as cl\n",
    "import subprocess\n",
    "import logging\n",
    "import torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@cl.cache\n",
    "def load_diagnosis_model():\n",
    "    try:\n",
    "        model_path = \"/content/unsloth.Q4_K_M.gguf\"\n",
    "        logger.info(f\"Model path: {model_path}\")\n",
    "        \n",
    "        return Llama(\n",
    "            model_path=model_path,\n",
    "            n_ctx=2048,\n",
    "            n_gpu_layers=15,\n",
    "            n_threads=8,\n",
    "            verbose=False\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Diagnosis model failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "@cl.cache \n",
    "def load_prescription_model():\n",
    "    try:\n",
    "        logger.info(\"Loading prescription model...\")\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "            max_seq_length=2048,\n",
    "            load_in_4bit=True,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        FastLanguageModel.for_inference(model)\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prescription model failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "@cl.set_starters\n",
    "async def set_starters():\n",
    "    return [\n",
    "        cl.Starter(\n",
    "            label=\"Detect Common Cold\",\n",
    "            message=\"I've been experiencing symptoms like sneezing, runny nose, nasal congestion, sore throat, cough, mild headache, slight fever, fatigue. What could it be?\",\n",
    "            ),\n",
    "\n",
    "        cl.Starter(\n",
    "            label=\"Detect Pneumonia\",\n",
    "            message=\"I've been experiencing symptoms like cough, fever, chills, shortness of breath, chest pain, fatigue, sweating, loss of appetite. What could it be?\",\n",
    "            ),\n",
    "        cl.Starter(\n",
    "            label=\"Detect Heart Attack\",\n",
    "            message=\"I've been experiencing symptomns like vomiting, breathlessness, sweating, and chest pain. What could it be?\",\n",
    "            ),\n",
    "        cl.Starter(\n",
    "            label=\"Detect Chicken Pox\",\n",
    "            message=\"I've been experiencing symptoms like itching, skin rash, fatigue, lethargy, high fever, headache, loss of appetite, mild fever, malaise, and red spots over body. What could this be?\",\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "@cl.on_chat_start\n",
    "async def start_chat():\n",
    "    try:\n",
    "        status = cl.Message(content=\"üöÄ Starting Medical AI...\")\n",
    "        await status.send()\n",
    "        \n",
    "        status.content = \"üì• Loading Diagnosis Model...\"\n",
    "        await status.update()\n",
    "        diag_model = load_diagnosis_model()\n",
    "        \n",
    "        status.content = \"üì• Loading Prescription Model...\"\n",
    "        await status.update()\n",
    "        rx_model, rx_tokenizer = load_prescription_model()\n",
    "        \n",
    "        status.content = \"üîç Verifying GPU Resources...\"\n",
    "        await status.update()\n",
    "        free, total = torch.cuda.mem_get_info()\n",
    "        logger.info(f\"GPU Memory - Free: {free/1e9:.1f}GB, Total: {total/1e9:.1f}GB\")\n",
    "        \n",
    "        if free < 2e9:\n",
    "            raise MemoryError(f\"Low GPU memory: {free/1e9:.1f}GB free\")\n",
    "            \n",
    "        cl.user_session.set(\"diag_model\", diag_model)\n",
    "        cl.user_session.set(\"rx_model\", rx_model)\n",
    "        cl.user_session.set(\"rx_tokenizer\", rx_tokenizer)\n",
    "        \n",
    "        status.content = \"ü©∫ AI Doctor Ready! Describe your symptoms.\"\n",
    "        await status.update()\n",
    "        \n",
    "    except Exception as e:\n",
    "        await cl.Message(content=f\"‚ùå Initialization Failed: {str(e)}\").send()\n",
    "        raise\n",
    "\n",
    "@cl.on_message\n",
    "async def main(message: cl.Message):\n",
    "    try:\n",
    "        diag_model = cl.user_session.get(\"diag_model\")\n",
    "        rx_model = cl.user_session.get(\"rx_model\")\n",
    "        rx_tokenizer = cl.user_session.get(\"rx_tokenizer\")\n",
    "        \n",
    "        response = cl.Message(content=\"\")\n",
    "        await response.send()\n",
    "        \n",
    "        # Step 1: Generate Diagnosis\n",
    "        logger.info(\"Generating diagnosis...\")\n",
    "        diagnosis = diag_model.create_completion(\n",
    "            prompt=f\"\"\"<|im_start|>system\n",
    "            Analyze symptoms and respond with:\n",
    "            - Detected Condition\n",
    "            - Confidence Level\n",
    "            - Key Indicators<|im_end|>\n",
    "            <|im_start|>user\n",
    "            {message.content}<|im_end|>\n",
    "            <|im_start|>assistant\"\"\",\n",
    "            max_tokens=256,\n",
    "            temperature=0.2,\n",
    "            stop=[\"<|im_end|>\"]\n",
    "        )['choices'][0]['text']\n",
    "        \n",
    "        logger.info(\"Generating treatment...\")\n",
    "        treatment = rx_tokenizer.decode(\n",
    "            rx_model.generate(\n",
    "                **rx_tokenizer(\n",
    "                    f\"\"\"<|im_start|>system\n",
    "                    Provide treatment plan based on diagnosis:\n",
    "                    {diagnosis}<|im_end|>\n",
    "                    <|im_start|>assistant\"\"\",\n",
    "                    return_tensors=\"pt\"\n",
    "                ).to(\"cuda\"),\n",
    "                max_new_tokens=512\n",
    "            )[0],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        response.content = f\"\"\"\n",
    "        **Diagnosis**\n",
    "        {diagnosis.strip()}\n",
    "        \n",
    "        {treatment.split('<|im_end|>')[-1].strip()}\n",
    "        \"\"\"\n",
    "        await response.update()\n",
    "        \n",
    "    except Exception as e:\n",
    "        await cl.Message(content=f\"‚ö†Ô∏è Error: {str(e)}\").send()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cl.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T16:56:21.293931Z",
     "iopub.status.busy": "2025-02-03T16:56:21.293593Z",
     "iopub.status.idle": "2025-02-03T16:56:31.721317Z",
     "shell.execute_reply": "2025-02-03T16:56:31.720083Z",
     "shell.execute_reply.started": "2025-02-03T16:56:21.293903Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "from pyngrok import ngrok\n",
    "\n",
    "process = subprocess.Popen([\"chainlit\", \"run\", \"ai-doctor.py\", \"--port\", \"8000\", \"--host\", \"0.0.0.0\"])\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "ngrok_tunnel = ngrok.connect(8000)\n",
    "print(\"Chainlit App URL:\", ngrok_tunnel.public_url)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
